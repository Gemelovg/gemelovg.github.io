[ { "title": "Wireguard for GCNAT to selfhost services.", "url": "/posts/Wireguard_for_CGNAT/", "categories": "", "tags": "wireguard, vpn, selfhosting, bash", "date": "2023-03-20 00:00:00 -0600", "snippet": "For this challenge we will use a fake credit card dataset that is attached as df.csv.which includes information from a public Kaggle dataset with three added fields: activated_date,last_payment_date and fraud.Going to start by importing the libraries im going to use.import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sns" }, { "title": "Kaggle challenge dataset.", "url": "/posts/kaggle_credit_card_datase/", "categories": "", "tags": "python, pandas, matplotlib, excel, plot", "date": "2023-03-20 00:00:00 -0600", "snippet": "For this challenge we will use a fake credit card dataset that is attached as df.csv.which includes information from a public Kaggle dataset with three added fields: activated_date,last_payment_date and fraud.Going to start by importing the libraries im going to use.import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsWe import import the file while parsing date columns we’ll use later.df = pd.read_csv('Stori_Data_Challenge.xls', index_col =0, parse_dates=['activated_date', 'last_payment_date'])df.head()We use describe method to get statistics and sumatized values. We can see that there are 8950 rows. In the balance column We can see that the min value is 0 and max 119043. This means values are spread by a big amount.df.describe()df.dtypescust_id objectactivated_date datetime64[ns]last_payment_date datetime64[ns]balance float64balance_frequency float64purchases float64oneoff_purchases float64installments_purchases float64cash_advance float64purchases_frequency float64oneoff_purchases_frequency float64purchases_installments_frequency float64cash_advance_frequency float64cash_advance_trx int64purchases_trx int64credit_limit float64payments float64minimum_payments float64prc_full_payment float64tenure int64fraud int64dtype: objectWe check for missing values. We will use cash_advance later so We will inpute it.df.isnull().sum()Looking good, but something seems off…cust_id 0activated_date 13last_payment_date 9balance 2balance_frequency 0purchases 0oneoff_purchases 0installments_purchases 0cash_advance 112purchases_frequency 0oneoff_purchases_frequency 0purchases_installments_frequency 0cash_advance_frequency 0cash_advance_trx 0purchases_trx 0credit_limit 1payments 0minimum_payments 321prc_full_payment 0tenure 0fraud 0dtype: int64df.fillna(method ='ffill', inplace = True) df.isnull().sum()Looking good, but something seems off…cust_id 0activated_date 0last_payment_date 0balance 0balance_frequency 0purchases 0oneoff_purchases 0installments_purchases 0cash_advance 0purchases_frequency 0oneoff_purchases_frequency 0purchases_installments_frequency 0cash_advance_frequency 0cash_advance_trx 0purchases_trx 0credit_limit 0payments 0minimum_payments 0prc_full_payment 0tenure 0fraud 0Lets create a histogram to chart how is the balance distributed. With this plot we can see that most people less than 5000 on their balance accounts. A good way to visualize this would be a boxplot.sns.set_theme()sns.displot(df.balance,bins=20)plt.show()The blue bar show where most of the values are located. Here we can see most of them are bellow 2500.sns.boxplot(y=df.balance, width=0.3)plt.show()Lets how many are there: About 80% of the balance are less than 2500balance = len(df[df.balance &lt; 2500])balance7099Report mean and median balance, grouped by year and month of activated_dateg = df.groupby([df.activated_date.dt.year, df.activated_date.dt.month])['balance'].agg([\"mean\", \"median\"])gReport in a table the following information for customers who activated their account and made their last payment during 2020: cust_id (excluding letters), activated_date (in format YYYY-MM), last_payment_date (in format YYYY-MM-DD), cash_advance, credit_limit, and a calculated field of cash_advance as a percentage of credit_limith = df[df.last_payment_date.dt.year == 2020]hi = h[df.activated_date.dt.year == 2020]ij = i[['cust_id', 'activated_date', 'last_payment_date', 'cash_advance', 'credit_limit' ]]jj.cust_id = j.cust_id.str.replace(r'\\D', '')j.activated_date = pd.to_datetime(j.activated_date, format='%Y-%m-%d')j.activated_date = j.activated_date.dt.strftime('%Y-%m')j.last_payment_date = pd.to_datetime(j.last_payment_date, format='%Y-%m-%d')j.last_payment_date = j.last_payment_date.dt.strftime('%Y-%m-%d')j['percentage'] = (j['cash_advance']*100).divide( j['credit_limit'])j" }, { "title": "Buying stocks or better not!", "url": "/posts/Buying_stocks/", "categories": "", "tags": "python, pandas, matplotlib, excel, plot", "date": "2022-09-03 00:00:00 -0500", "snippet": "Using pandas and yahoo finance to plot stock data.As someone how likes to lose money on the stock market, Im always looking for ways to make it happend faster. I just found out about the yahoo finance library for python.Going to start by importing said yahoo libraryimport yfinance as yfimport pandas as pdIt works like this. You resquest the infomartion from a ticker with start and the end date. Lets look at NVIDIA NVDA. I bought arround november 2021Read File and rename Colums to something easier to read.NVDA = yf.download(\"NVDA\", start=\"2020-01-01\", end=\"2022-09-03\")NVDA.head()It looks something like this\tOpen\tHigh\tLow\tClose\tAdj Close\tVolumeDate\t\t\t\t\t\t2020-01-02\t59.687500\t59.977501\t59.180000\t59.977501\t59.803612\t237536002020-01-03\t58.775002\t59.457500\t58.525002\t59.017502\t58.846394\t205384002020-01-06\t58.080002\t59.317501\t57.817501\t59.264999\t59.093170\t262636002020-01-07\t59.549999\t60.442501\t59.097500\t59.982498\t59.808590\t314856002020-01-08\t59.939999\t60.509998\t59.537498\t60.095001\t59.920769\t27710800Looking good, lets add a new column, change that will move the period by 365 days to compare the price from the previous year.NVDA['change'] = NVDA.Close.pct_change(365)NVDA['diff'] = NVDA.Close - NVDA.changeNVDA.fillna(0)Open\tHigh\tLow\tClose\tAdj Close\tVolume\tchange\tdiffDate\t\t\t\t\t\t\t\t2020-01-02\t59.687500\t59.977501\t59.180000\t59.977501\t59.803612\t23753600\t0.000000\t0.0000002020-01-03\t58.775002\t59.457500\t58.525002\t59.017502\t58.846394\t20538400\t0.000000\t0.0000002020-01-06\t58.080002\t59.317501\t57.817501\t59.264999\t59.093170\t26263600\t0.000000\t0.0000002020-01-07\t59.549999\t60.442501\t59.097500\t59.982498\t59.808590\t31485600\t0.000000\t0.0000002020-01-08\t59.939999\t60.509998\t59.537498\t60.095001\t59.920769\t27710800\t0.000000\t0.000000...\t...\t...\t...\t...\t...\t...\t...\t...2022-08-29\t160.199997\t163.380005\t157.669998\t158.009995\t158.009995\t49613200\t0.241973\t157.7680222022-08-30\t159.600006\t160.389999\t151.820007\t154.679993\t154.679993\t53018100\t0.204134\t154.4758592022-08-31\t153.839996\t155.399994\t149.589996\t150.940002\t150.940002\t57371000\t0.144677\t150.7953252022-09-01\t142.089996\t143.800003\t132.699997\t139.369995\t139.369995\t117886500\t0.066274\t139.3037212022-09-02\t141.000000\t141.710007\t135.910004\t136.470001\t136.470001\t74259000\t0.079412\t136.390590Looking good, lets plot.NVDA['diff'].plot()So, from a all time high near the end of 2021, the stock is now sitting at 136 dollars, about 58% loss. Not a great look.Lets try Netflix an other tech gigant. Its ticker is NFLX and will plot it along side Nvidia.NFLX = yf.download(\"NFLX\", start=\"2020-01-01\", end=\"2022-09-03\")NFLX['change'] = NFLX.Close.pct_change(365)NFLX['diff'] = NFLX.Close - NFLX.changeNVDA['diff'].plot()NFLX['diff'].plot()Neflix had an all time high of 690 dollars, now sitting at 226, that is a 67% loss. Tech isnt looking good. Lets look at something diffent. Retail. Costco is a big player. Lets plot.COST = yf.download(\"COST\", start=\"2020-01-01\", end=\"2022-09-03\")COST['change'] = COST.Close.pct_change(365)COST['diff'] = COST.Close - COST.changeNVDA['diff'].plot()NFLX['diff'].plot()COST['diff'].plot()We can see Costco has been very stable compared to other tech companies, if fact if you boought at the sametime I did, You are in the green.Lets try an ETF, ETF are a pool of stocks that is traded the same way as stocks, so instead of buying Nvidia, Intel and Apple stocks, You can by QQQ that has all 3 and more.Lets try XLE. It traks the energy sector of the S&amp;P500XLE = yf.download(\"XLE\", start=\"2020-01-01\", end=\"2022-09-03\")XLE['change'] = XLE.Close.pct_change(365)XLE['diff'] = XLE.Close - XLE.changeNVDA['diff'].plot()NFLX['diff'].plot()COST['diff'].plot()XLE['diff'].plot()Even though the scale is not right, We can see this stock has increased its value.Thus diversification is the best to invest.Full notebook athttps://app.datacamp.com/workspace/w/175aa01c-4626-483a-8d88-70060fed4c14/edit" }, { "title": "Where is my money going.", "url": "/posts/Where_is_my_money_going-copy/", "categories": "", "tags": "python, pandas, matplotlib, excel, plot", "date": "2022-08-20 00:00:00 -0500", "snippet": "Where is my money going.I think I have been spending a lot of my money on Amazon but I am not sure, lets take a look.Going to start by importing the libraries im going to use.import pandas as pdimport matplotlib.pyplot as pltRead File and rename Colums to something easier to read.bank = pd.read_excel('excel.xlsx')colums=['Date', 'Description', 'Deposits','Price']bank.columns=columsbank.head()It looks something like thisDate\tDescription\tDeposits\tPrice0\t27 Jul 2022|7\tSORIANA 688 URBANIA\tNaN\t$ 77.901\t27 Jul 2022|7\tGAS EST SERV P UNIV 2\tNaN\t$ 613.702\t27 Jul 2022|7\tPAYPAL ETNTURISTAR\tNaN\t$ 1,560.003\t25 Jul 2022|7\tHOME DEPOT CORDILLERAS\tNaN\t$ 431.914\t25 Jul 2022|7\tPP*FASTSPRING\tNaN\t$ 223.77Lets remove the Deposits column so people doesnt know much I make and We are not going to use it anyway.bank = bank[['Date', 'Description','Price']]bank.head()Looking good, but something seems off…\tDate\tDescription\tPrice0\t27 Jul 2022|7\tSORIANA 688 URBANIA\t$ 77.901\t27 Jul 2022|7\tGAS EST SERV P UNIV 2\t$ 613.702\t27 Jul 2022|7\tPAYPAL ETNTURISTAR\t$ 1,560.003\t25 Jul 2022|7\tHOME DEPOT CORDILLERAS\t$ 431.914\t25 Jul 2022|7\tPP*FASTSPRING\t$ 223.77Lets sum the whole colum and see how much I am spending…suma = bank['Price'].sum()&gt;Output exceeds the size limit. Open the full output data in a text editor---------------------------------------------------------------------------&gt;TypeError Traceback (most recent call last)c:\\Users\\gemel\\OneDrive\\Documentos\\bank\\bank.ipynb Cell 7 in &lt;cell line: 1&gt;()----&gt; 1 suma = bank['Price'].sum()... 46 def _sum(a, axis=None, dtype=None, out=None, keepdims=False, 47 initial=_NoValue, where=True):---&gt; 48 return umr_sum(a, axis, dtype, out, keepdims, initial, where)&gt;TypeError: can only concatenate str (not \"int\") to strOh no, the column is not numeric, lets fix that.Numbers on the column look like this: $ 1,560.00Lets remove the dollar sign, and the commas.bank['Price'] = bank['Price'].str.replace('[$,]','').astype(float)bank['Price']0 77.901 613.702 1560.003 431.914 223.775 682.766 1588.007 170.008 182.109 242.8810 11.0511 NaN12 163.0013 323.5014 269.99We also have some empty spaces (not a number) from the deposits column. Thets get rid of them.bank.dropna(inplace=True)bankDate\tDescription\tPrice0\t27 Jul 2022|7\tSORIANA 688 URBANIA\t77.901\t27 Jul 2022|7\tGAS EST SERV P UNIV 2\t613.702\t27 Jul 2022|7\tPAYPAL ETNTURISTAR\t1560.003\t25 Jul 2022|7\tHOME DEPOT CORDILLERAS\t431.914\t25 Jul 2022|7\tPP*FASTSPRING\t223.775\t24 Jul 2022|7\tUBER* EATS\t682.766\t23 Jul 2022|7\tHOME DEPOT\t1588.007\t22 Jul 2022|7\tDomains\t170.008\t22 Jul 2022|7\tSORIANA 688 URBANIA\t182.109\t21 Jul 2022|7\tUBER* EATS\t242.8810\t19 Jul 2022|7\tPAYPAL *KINGUINDIGI\t11.0512\t17 Jul 2022|7\tACNT*CINESONLINE\t163.0013\t17 Jul 2022|7\tWAL MART VALLARTA\t323.5014\t17 Jul 2022|7\tPAYPAL STEAM GAMES\t269.9916\t16 Jul 2022|7\tAMAZON MX MSI\t1366.9817\t16 Jul 2022|7\tAMAZON MX MSI\t370.4518\t16 Jul 2022|7\t**/TICKETMASTER MEXIC\t1322.7619\t16 Jul 2022|7\tSAMS VENTA EN LINEA\t970.8620\t16 Jul 2022|7\tSAMS VENTA EN LINEA\t255.6721\t16 Jul 2022|7\tPAYPAL CHEILMEXICO\t1031.2122\t16 Jul 2022|7\t**/TICKETMASTER\t339.6723\t16 Jul 2022|7\tAMAZON MX MKTPLACE MSI\t550.4224\t16 Jul 2022|7\tMERCADO PAGO 1\t341.1125\t16 Jul 2022|7\tAMAZON MX MKTPLACE MSI\t470.4226\t16 Jul 2022|7\tMERCADO PAGO 4\t250.9827\t16 Jul 2022|7\tREST RANCHERO BARBACOA\t92.5028\t16 Jul 2022|7\tUBER* EATS\t244.6533\t14 Jul 2022|7\tAMAZON MX\t1159.00No NaN columns, nice.We have an other problem. The Date column has a pipe and a number 7 at the end.Lets remove them and convert it to datetimebank['Date'] = bank.apply(lambda x: x['Date'][:-2], axis=1)bank['Date'] = pd.to_datetime(bank['Date'])bank['Date'].head()0 2022-07-271 2022-07-272 2022-07-273 2022-07-254 2022-07-25Lets use the Description column to count the number of different purchases I have done.bank.Description.value_counts()UBER* EATS 3SORIANA 688 URBANIA 2AMAZON MX MSI 2AMAZON MX MKTPLACE MSI 2SAMS VENTA EN LINEA 2REST RANCHERO BARBACOA 1MERCADO PAGO 4 1MERCADO PAGO 1 1**/TICKETMASTER 1PAYPAL CHEILMEXICO 1**/TICKETMASTER MEXIC 1PAYPAL STEAM GAMES 1GAS EST SERV P UNIV 2 1WAL MART VALLARTA 1ACNT*CINESONLINE 1PAYPAL *KINGUINDIGI 1Domains 1HOME DEPOT 1PP*FASTSPRING 1HOME DEPOT CORDILLERAS 1PAYPAL ETNTURISTAR 1AMAZON MX 1Name: Description, dtype: int64Wow. Even though we know some expenses are form the same store, since they are have exactly the same name, the dataframe count them as different. Let use just the first word as the name of the Store.Now lets create a new column name Store and get the first word from the string.bank['Store'] = bank.Description.str.split().str.get(0)bank['Store']Lets get the count againbank.Store.value_counts()AMAZON 5PAYPAL 4UBER* 3SORIANA 2HOME 2**/TICKETMASTER 2SAMS 2MERCADO 2GAS 1PP*FASTSPRING 1Domains 1ACNT*CINESONLINE 1WAL 1REST 1Name: Store, dtype: int64A lot of Amazon and Paypal, lets see the total amountstotal = bank.groupby('Store').sum().sort_values('Price', ascending=False).reset_index()totalStore\tPrice0\tAMAZON\t3917.271\tPAYPAL\t2872.252\tHOME\t2019.913\t**/TICKETMASTER\t1662.434\tSAMS\t1226.535\tUBER*\t1170.296\tGAS\t613.707\tMERCADO\t592.098\tWAL\t323.509\tSORIANA\t260.0010\tPP*FASTSPRING\t223.7711\tDomains\t170.0012\tACNT*CINESONLINE\t163.0013\tREST\t92.50That seems to be a lot.Lets plot to take a better look.plt.pie(total.Price, labels=total.Store, autopct='%.0f%%', shadow=True, rotatelabels='true')plt.showAlmost half of the charges are from Amazon or Paypal(probably ordering to go).yeahNow We know what I should spend less time doing!Full notebook athttps://app.datacamp.com/workspace/w/175aa01c-4626-483a-8d88-70060fed4c14/edit" }, { "title": "Where is my money going.", "url": "/posts/Where_is_my_money_going/", "categories": "", "tags": "python, pandas, matplotlib, excel, plot", "date": "2022-08-20 00:00:00 -0500", "snippet": "Where is my money going.I think I have been spending a lot of my money on Amazon but I am not sure, lets take a look.Going to start by importing the libraries im going to use.import pandas as pdimport matplotlib.pyplot as pltRead File and rename Colums to something easier to read.bank = pd.read_excel('excel.xlsx')colums=['Date', 'Description', 'Deposits','Price']bank.columns=columsbank.head()It looks something like thisDate\tDescription\tDeposits\tPrice0\t27 Jul 2022|7\tSORIANA 688 URBANIA\tNaN\t$ 77.901\t27 Jul 2022|7\tGAS EST SERV P UNIV 2\tNaN\t$ 613.702\t27 Jul 2022|7\tPAYPAL ETNTURISTAR\tNaN\t$ 1,560.003\t25 Jul 2022|7\tHOME DEPOT CORDILLERAS\tNaN\t$ 431.914\t25 Jul 2022|7\tPP*FASTSPRING\tNaN\t$ 223.77Lets remove the Deposits column so people doesnt know much I make and We are not going to use it anyway.bank = bank[['Date', 'Description','Price']]bank.head()Looking good, but something seems off…\tDate\tDescription\tPrice0\t27 Jul 2022|7\tSORIANA 688 URBANIA\t$ 77.901\t27 Jul 2022|7\tGAS EST SERV P UNIV 2\t$ 613.702\t27 Jul 2022|7\tPAYPAL ETNTURISTAR\t$ 1,560.003\t25 Jul 2022|7\tHOME DEPOT CORDILLERAS\t$ 431.914\t25 Jul 2022|7\tPP*FASTSPRING\t$ 223.77Lets sum the whole colum and see how much I am spending…suma = bank['Price'].sum()&gt;Output exceeds the size limit. Open the full output data in a text editor---------------------------------------------------------------------------&gt;TypeError Traceback (most recent call last)c:\\Users\\gemel\\OneDrive\\Documentos\\bank\\bank.ipynb Cell 7 in &lt;cell line: 1&gt;()----&gt; 1 suma = bank['Price'].sum()... 46 def _sum(a, axis=None, dtype=None, out=None, keepdims=False, 47 initial=_NoValue, where=True):---&gt; 48 return umr_sum(a, axis, dtype, out, keepdims, initial, where)&gt;TypeError: can only concatenate str (not \"int\") to strOh no, the column is not numeric, lets fix that.Numbers on the column look like this: $ 1,560.00Lets remove the dollar sign, and the commas.bank['Price'] = bank['Price'].str.replace('[$,]','').astype(float)bank['Price']0 77.901 613.702 1560.003 431.914 223.775 682.766 1588.007 170.008 182.109 242.8810 11.0511 NaN12 163.0013 323.5014 269.99We also have some empty spaces (not a number) from the deposits column. Thets get rid of them.bank.dropna(inplace=True)bankDate\tDescription\tPrice0\t27 Jul 2022|7\tSORIANA 688 URBANIA\t77.901\t27 Jul 2022|7\tGAS EST SERV P UNIV 2\t613.702\t27 Jul 2022|7\tPAYPAL ETNTURISTAR\t1560.003\t25 Jul 2022|7\tHOME DEPOT CORDILLERAS\t431.914\t25 Jul 2022|7\tPP*FASTSPRING\t223.775\t24 Jul 2022|7\tUBER* EATS\t682.766\t23 Jul 2022|7\tHOME DEPOT\t1588.007\t22 Jul 2022|7\tDomains\t170.008\t22 Jul 2022|7\tSORIANA 688 URBANIA\t182.109\t21 Jul 2022|7\tUBER* EATS\t242.8810\t19 Jul 2022|7\tPAYPAL *KINGUINDIGI\t11.0512\t17 Jul 2022|7\tACNT*CINESONLINE\t163.0013\t17 Jul 2022|7\tWAL MART VALLARTA\t323.5014\t17 Jul 2022|7\tPAYPAL STEAM GAMES\t269.9916\t16 Jul 2022|7\tAMAZON MX MSI\t1366.9817\t16 Jul 2022|7\tAMAZON MX MSI\t370.4518\t16 Jul 2022|7\t**/TICKETMASTER MEXIC\t1322.7619\t16 Jul 2022|7\tSAMS VENTA EN LINEA\t970.8620\t16 Jul 2022|7\tSAMS VENTA EN LINEA\t255.6721\t16 Jul 2022|7\tPAYPAL CHEILMEXICO\t1031.2122\t16 Jul 2022|7\t**/TICKETMASTER\t339.6723\t16 Jul 2022|7\tAMAZON MX MKTPLACE MSI\t550.4224\t16 Jul 2022|7\tMERCADO PAGO 1\t341.1125\t16 Jul 2022|7\tAMAZON MX MKTPLACE MSI\t470.4226\t16 Jul 2022|7\tMERCADO PAGO 4\t250.9827\t16 Jul 2022|7\tREST RANCHERO BARBACOA\t92.5028\t16 Jul 2022|7\tUBER* EATS\t244.6533\t14 Jul 2022|7\tAMAZON MX\t1159.00No NaN columns, nice.We have an other problem. The Date column has a pipe and a number 7 at the end.Lets remove them and convert it to datetimebank['Date'] = bank.apply(lambda x: x['Date'][:-2], axis=1)bank['Date'] = pd.to_datetime(bank['Date'])bank['Date'].head()0 2022-07-271 2022-07-272 2022-07-273 2022-07-254 2022-07-25Lets use the Description column to count the number of different purchases I have done.bank.Description.value_counts()UBER* EATS 3SORIANA 688 URBANIA 2AMAZON MX MSI 2AMAZON MX MKTPLACE MSI 2SAMS VENTA EN LINEA 2REST RANCHERO BARBACOA 1MERCADO PAGO 4 1MERCADO PAGO 1 1**/TICKETMASTER 1PAYPAL CHEILMEXICO 1**/TICKETMASTER MEXIC 1PAYPAL STEAM GAMES 1GAS EST SERV P UNIV 2 1WAL MART VALLARTA 1ACNT*CINESONLINE 1PAYPAL *KINGUINDIGI 1Domains 1HOME DEPOT 1PP*FASTSPRING 1HOME DEPOT CORDILLERAS 1PAYPAL ETNTURISTAR 1AMAZON MX 1Name: Description, dtype: int64Wow. Even though we know some expenses are form the same store, since they are have exactly the same name, the dataframe count them as different. Let use just the first word as the name of the Store.Now lets create a new column name Store and get the first word from the string.bank['Store'] = bank.Description.str.split().str.get(0)bank['Store']Lets get the count againbank.Store.value_counts()AMAZON 5PAYPAL 4UBER* 3SORIANA 2HOME 2**/TICKETMASTER 2SAMS 2MERCADO 2GAS 1PP*FASTSPRING 1Domains 1ACNT*CINESONLINE 1WAL 1REST 1Name: Store, dtype: int64A lot of Amazon and Paypal, lets see the total amountstotal = bank.groupby('Store').sum().sort_values('Price', ascending=False).reset_index()totalStore\tPrice0\tAMAZON\t3917.271\tPAYPAL\t2872.252\tHOME\t2019.913\t**/TICKETMASTER\t1662.434\tSAMS\t1226.535\tUBER*\t1170.296\tGAS\t613.707\tMERCADO\t592.098\tWAL\t323.509\tSORIANA\t260.0010\tPP*FASTSPRING\t223.7711\tDomains\t170.0012\tACNT*CINESONLINE\t163.0013\tREST\t92.50That seems to be a lot.Lets plot to take a better look.plt.pie(total.Price, labels=total.Store, autopct='%.0f%%', shadow=True, rotatelabels='true')plt.showAlmost half of the charges are from Amazon or Paypal(probably ordering to go).yeahNow We know what I should spend less time doing!Full notebook athttps://app.datacamp.com/workspace/w/175aa01c-4626-483a-8d88-70060fed4c14/edit" } ]
